# x9115222

# Repository for CS 591(Automated Software Engineering)

Contributors:-

  Bhanu Anand(bhanuanand28)
  
  Esha Sharma(eshasharma)
  
  Vinay Todalge (vntodalge)

_____________________________________________________________________________________________________________________________

##Hyper Parameter Optimization

### Running Instructions 
  1. Clone the github repository x9115222 from git@github.com:bhanuanand28/x9115222.git
  2. Navigate to ./x9115222/hw/code/10 
  3. run testGA.py
  
###Abstract
In this study we used Differential Evolution(DE) to generate the set of values which decide the extend of mutation ,crossover , 
the number of candidates and the number if generations that a genetic algorithm(GA) will run. We use DE to tune our default control 
settings. The DE tunes the GA so that the set of values which performs the most efficiently optimized solution for different 
DTLZs get generated. We apply DE to tune our GAs which optimise DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 
, 40 decision. We observe that tuning the GA via DE improves the performance of GA on DTLZ3 and DTLZ5 with 4,6,8 objectives and DTLZ7
for 8 objectives. 

###Introduction and Background
When we use Genetic Algorithm to optimise a problem ,there are a number of parameters which decide the extent of mutation ,
crossover etc. These parameters greatly influence the performance of a GA . In the previous code we used default values for this 
optimisation. In this study we apply Differential Evolution and generate different sets of default parameter settings. These are 
then used to run GA on DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 , 40 decisions. We analyze the performance 
of DE on the GAs by comparing the hypervolumes generated before and after the tuning is done.

###Genetic Algorithm 
A Genetic Algorithm is a optimization algorithm which mimics the process of natural selection. In a genetic algorithm we use selection 
to generate the best population, we then use mutationa and crosssover with the default probabilty to generate children. The children
are then compared to the parents to see if the population is getting evolved for better. Like natural selection , this process uses
crossover, mutation and selection. 

###Differential Evolution 
Differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to 
a given measure of quality. DE optimizes a problem by maintaining a population of candidate solutions and creating new candidate 
solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best 
score or fitness on the optimization problem at hand. In this way the optimization problem is treated as a black box that merely 
provides a measure of quality given a candidate solution and the gradient is therefore not needed.

###Implementation 
To tune the GA via DE , we passed an object of GA to DE. The candidate function of the GA created a frontier of parameter lists. The 
DE tuned the GA for the parameter lists passed. 
The default parameter lists generated by the candidate solution for the GA were generated by picking values in these ranges and then 
generating 20 candidates for those . 
the max and min values for each of the default parameters were: 
Probabilty for mutation: 0.01 to 0.09
Probabilty for crossover: 0.50 to 0.99
number of lives: 5
number of candidates: 100 to 500 
number of generations: 100 to 300  
frontier_distribution=0.4 to 0.9

For each set of default parameters passed, the GA ran on the model passed and generated the final pareto frontier. This final pareto 
frontier was compared to the initial pareto frontier and the loss value was calculated and returned. On each iteration of the DE, the 
default parameter list created was evaluated using this loss value. If there was an improvement in the loss value it means that there
was a significant improvement from the first to the last iteration of the GA and hence the DE was doing a good job with the optimization
. Also , the GA was run on DTLZ 1 , 3, 5 , 7 each time with 2,4,6,8 objectives and 10, 20 , 40 decisions Every time the DE optimized
the GA for each of these models , it calculated the hypervolume for the last frontier generated by the GA. 

To check whether or not the DE tunes the GA well , we stored the hypervolumes for each run of GA on DTLZ 1 , 3, 5 , 7 each time with 
2,4,6,8 objectives and 10, 20 , 40 decisions and plotted this on a graph. 

We also ran the untuned GAs which we had generated in Code 9 and compared the graphs so as to compar eth performance before and after 
the GA was applied on.
The paramters for this run were :  
Probabilty for mutation: 0.05
Probabilty for crossover: 0.98 
number of lives: 5
number of candidates: 100
number of generations: 200 (but have early termination considered every 100 generations)
frontier_distribution=0.8

The values for the range number of candidates was kept 200 for the untuned optimizer and between 100 to 300 for the tuned 
optimizer so as to limit the time needed to run the code . 

Similarly the default parameter for the DE were limited to 
 number of generations = 10
 number of candidates =20 
so as to save on the runninf time. 

###Results

For the different DTLZs with different number of objectives and decisions we have calculated the hypervolume and plotted it here.
For each DTLZ model the plot can be seen below . The number under the plot represents the number of objectives, while the 
number 10, 20 , 40 represents the number of decisions. 

####DTLZ1 - before and after optimization

Below is the plot for DTLZ1 for the different number of objectives and decisions. We can see from the graph that the hypervolume
numbers are getting reduced . Hence we can conclude that the DE doesn't optimize the GA for DLTZ1 .

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ1.jpg)

####DTLZ3 - before and after optimization

Below is the plot for DTLZ3 for the different number of objectives and decisions. We can see from the graph that the hypervolume
numbers are getting improved significantly upto factor of 2 to 3 for 4,6,8 objectives. However this change is not very 
significant for 2 objectives. Hence we can conclude that the DE optimizes the GA for DLTZ3 with 4,6,8 objectives to a significant
extent but not for 2 objectives.

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ3.jpg)

####DTLZ5 - before and after optimization

Below is the plot for DTLZ5 for the different number of objectives and decisions.We can see from the graph that the hypervolume
numbers are getting improved significantly upto factor of 2 to 3 for 4,6,8 objectives. However this change is not very 
significant for 2 objectives. Hence we can conclude that the DE optimizes the GA for DLTZ5 with 4,6,8 objectives to a significant
extent but not for 2 objectives.

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ5.jpg)

####DTLZ7 - before and after optimization

Below is the plot for DTLZ7 for the different number of objectives and decisions. We can see from the graph that the hypervolume
numbers are getting improved significantly upto factor of 1 to 1.5 for 8 objectives . For 6 objectives , on an average this number
gets reduced. While there is no change for 2, 4 objectives .Hence we can conclude that the DE optimizes the GA for DLTZ7
to a significant extent for 8 objectives.Also , this number gets worsened for DTLZ7 with 2 objectives.

![alt tag](https://github.com/bhanuanand28/x9115222/blob/master/hw/code/10/ScreenShots/DTLZ7.jpg)

It can be observed from the graphs that optimizing the GA through DE produced a significant improvement in the 
hypervolume values for some of the DTLZ cases but not for all.

###Conclusions
We can hence conclude that GA optimizes DE to a significant extent for DTLZ3 and DTLZ5 with 4,6,8 objectives and DTLZ7
for 8 objectives.
Tuning with DE worsens optimzation achieved through DE for DTLZ1 and DTLZ7 for 2 objectives. 

###Threats to Validity 
1. We ran the code only for the given default values for DE. We could run the code for different sets of default values and try to 
   decide which set of default paramters works the best. 
2. We use Hypervolume function to measure performance. There are multiple other tools which could be used. The given results might 
   be skewed because of the use of Hypervolume. 
3. The range of values for generating candidate solutions for GA's default settings were very narrow. 
4. The code was run for a very limited number of iterations of both GA and DE because of time constraints.

###Future Work 
1. The code could be run for more number of iterations for DE and for GA. 
2. Instead of just using hypervolume , we could also use spread or the loss and compute multiple graphs. Instead on measuring 
   performance based on just hypervolume , we could base it on all three of the parameters and then conclude results.
3. The range of values to generate candidate solutions can be made more wide. 


###References:-

 1. [Pseudo-code for Genetic Algorithm](http://www.cleveralgorithms.com/nature-inspired/evolution/genetic_algorithm.html)
 2. Book : Clever Algorithms by Jason Brownlee
 3. https://github.com/txt/mase/blob/master/lessthan.md
 4. https://github.com/txt/mase/blob/master/STATS.md
 5. https://en.wikipedia.org/wiki/Genetic_algorithm
 6. https://github.com/txt/mase/blob/master/DE.md


###Acknowledgements

   The study uses code found here :
 1.  This study uses code for Scott Knott given here : https://github.com/txt/mase/blob/master/src/doc/sk.py
 2.  This study used Hypervolume functions given here: 
     [Hypervolume Calculator](https://github.com/ai-se/storm/tree/master/PerformanceMetrics) 


